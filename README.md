<!-- ABOUT THE PROJECT -->
## Project Introduction and Rationale

The FiveThirtyEight Article [‚ÄúCollege Students Aren‚Äôt The Only Ones Abusing Adderall‚Äù](https://fivethirtyeight.com/features/college-students-arent-the-only-ones-abusing-adderall/) notes that there is a need to better understand to the risk factors behind recreational stimulant use and abuse among college students. One of the key findings of the FiveThirtyEight analysis was that students at more selective institutions are more likely to report that the non-prescription use of stimulants as study drugs was popular on campus, making the opportunity to understand the extent to which this FiveThirtyEight claim regarding non-prescription stimulant use at selective institutions replicates at Dartmouth intriguing. 

The project sought to identify specific demographic risk factors that may induce a Dartmouth student to abuse stimulants. This question was addressed by a number of methods, including a linear regression with multiple predictors and regression analyses of individual risk factors which appear to be of greatest importance in the broader analysis. By digging deeper and building the models described in the methods, our project transforms a topic of frequent casual discussion with an immense impact on student lives into actionable insights for those involved in student wellbeing. 

## Methods

### Data Collection

The project‚Äôs method of data collection was through the use of an anonymous survey. The survey asked for the following demographic information: class year, race, sex, ethnicity, LGBTQIA+ identification, FGLI identification, major identification, Greek affiliation, and varsity athletic status. The race and major identification categories allowed respondents to choose multiple categories, while the rest required students to choose either yes/no or a single category. Furthermore, the survey asked the two core questions underlying the analyses: whether respondents used stimulants that were not prescribed to them in the past 6 months, and whether respondents believed the non-prescription use of stimulants as "study drugs" is popular on campus. Both of these questions were binary, yes/no questions, which follows the methodology of the survey conducted by FiveThirtyEight. Finally, the survey had an optional open-ended asking for student input for the Student Wellness center or campus leadership, in support of the project‚Äôs vision of providing actionable insight for key stakeholders in student wellbeing.  

### Data Analysis
**The logistic regression methodology was as follows:**
   1. Constructing a logistic regression model on multiple predictors
   2. Looking at the specific categorical factors with the seemingly greatest correlation with stimulant use and belief in stimulant use, running two logistic regressions with just this predictor and both stimulant use and belief in stimulant use to further discern whether the data point had an effect 
   3. For each of these categorical features of interest, we ran two logistic regressions with just this predictor and both stimulant use and belief in stimulant use to further discern whether the data point had an effect 

This methodology mirrors that of the analyses of categorical variables conducted by the authors of the original FiveThirtyEight analysis. For instance, the authors of the FiveThirtyEight analysis created binary categorical variables for suicidal ideation among respondents (a variable titled ‚Äúseriously_thought_about_killing_oneself_last_year‚Äù) and then ran a logistic regression on it against their binary stimulant use and stimulant belief variables, just as this analysis does for a multitude of categorical predictors. 

**Applying data simulations to understand population effect sizes given the small survey sample size: **

Considering that the sample size was too small to discern whether the correlations  observed were statistically significant, the methodology also included a number of simulations of data which assisted in understanding the effects observed, particularly with regard to the potential for these effects to be statistically significant if observed at larger sample sizes. The goal of these methods was to understand a necessary sample size for observing a statistically significant correlation between the categorical predictors we included with largest effect and stimulant use or belief. 

1. First, using the properties of Bernoulli random variables, I estimated the standard error associated with the observed effect size of Greek life on stimulant use and belief in stimulant popularity. In order to run this estimation of standard error using properties of Bernoulli random variables, it was necessary to obtain estimates of the *q1* and *q0* parameters by putting the data into groups where x=1 and x=0, to then estimated the *q*s from the fractions of the data in each group as an estimate of effect size. 
2. The second simulation-based method for understanding whether the correlations observed were statistically significant was through generating simulated data for stimulant use and fitting a logistic regression to it, by splitting generated data points by class year and Greek affiliation, and then using the fractions of each group which used stimulants or believed stimulant use was popular to randomly assign a binary value to each. Then, plotting the standard error of the slope and p-value of the slope versus the sample size from the fitted logistic regression allowed stronger understadning of the significance of the effect observed and helped discern a sample size necessary in future experiments which seek to replicate these methods if similar effect sizes are present. 

## Results
* In our multivariate logistic regression for stimulant use, there was a pseudo-R^2 of 0.45. Roughly speaking, this shows the logistic regression doesn't do a great job at explaining what fraction of the variation in ùë¶ values is explained by the predictors. 
* 
* It is also important to note that we don't have the usual notion of residuals in this case, as we are using logistic regression. Instead, we compare how likely it is to see the particular sequence of ùë¶ values under the model vs. how likely it would be to see them if the chance to get ùë¶=1 did not depend on ùë•. Ideally speaking, this model is far from perfect, and tweaking our sampling techniques and having a larger sample size were primary ways to ensure a more precise model. Similar to the approach in the FiveThirtyEight article, after doing a logistic regression with multiple predictors, we performed logistic regressions with FGLI and Greek affiliation on the use of stimulants. We observe that FGLI status and Greek affiliation as predictors for stimulant use have a p-value below 0.05. However, we notice in both cases a standard error that is above 1. Meaning that if we were to expand our interval, say to 97.5% confidence, we wouldn't be so sure anymore that these predictors were significant, since the range of beta values when looking at the 97.5% CI will contain 0. While these predictors are intriguing, and helped give us more reason to believe that Greek affiliation would be positively correlated with stimulant use, our survey data is not enough to concretely make a statistically significant claim considering the standard error and p-values.
We then ran logistic regressions similarly but using belief in stimulant usage as the response variable instead of actual stimulant usage. Interestingly, we found that the predictors for third-year students and Greek affiliation were the most significant, having a range that did not include 0 within the 95% confidence interval for the slope. However, we cannot say that these predictors have a statistically significant effect on the outcome due to analogous reasoning concerning the high standard errors. We then ran logistic regressions with one predictor on Greek affiliation and being a third-year, seeing similar results as we did in the larger model with several predictors. We find that the belief model does not do a great job at predicting the response variables considering a very low pseudo R^2 of 0.13. 
Since our models were limited in their predictive power, especially considering the small sample size of our survey, we sought to use simulated data to understand the potential significance of similar effect size with larger sample sizes. In doing so, we can attempt to obtain an estimate of the sample size researchers may want to use when replicating similar experiments. Focusing on Greek affiliation as a predictor, we used the fraction of affiliated and unaffiliated respondents who used stimulants or believed in their popularity to generate simulated data. For our simulated data with these probabilities, we plotted the outcome of standard error and p values for both our response variables with simulated data. We observed the simulated standard error and p value as the sample size approaches the undergraduate population at Dartmouth. We observed that at approximately n=800, the p-value for both belief and usage approaches 0.000 and the standard error begins to slow down much more slowly. This is approximately ‚Öï of the Dartmouth student population. It is important to note that p-value is limited as a measure of significance at large sample sizes considering the manner in which it is calculated. The results with the standard error of the predictor coefficient shrinking in addition to the p-value, however, combine to demonstrate that with a large enough n, observing similar results as in our survey with a small sample size may lead to statistically significant results.


## Built With

* [VHDL](https://www.seas.upenn.edu/~ese171/vhdl/vhdl_primer.html) - The VHSIC Hardware Description Language is a hardware description language that can model the behavior and structure of digital systems at multiple levels of abstraction, ranging from the system level down to that of logic gates, for design entry, documentation, and verification purposes

